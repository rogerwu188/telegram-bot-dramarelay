#!/usr/bin/env python3
"""
é“¾æ¥éªŒè¯æ¨¡å—
ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–éªŒè¯è§†é¢‘é“¾æ¥çš„æè¿°å’Œæ ‡ç­¾ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
"""
import os
import re
import logging
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout

logger = logging.getLogger(__name__)

class LinkVerifier:
    """è§†é¢‘é“¾æ¥éªŒè¯å™¨ï¼ˆä½¿ç”¨ Playwright å¼‚æ­¥ APIï¼‰"""
    
    def __init__(self, screenshots_dir="/tmp/screenshots"):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.screenshots_dir = screenshots_dir
        os.makedirs(screenshots_dir, exist_ok=True)
    
    async def verify_link(self, url: str, task_title: str, task_description: str, timeout: int = 20000) -> dict:
        """
        éªŒè¯è§†é¢‘é“¾æ¥ - æ£€æŸ¥æè¿°å’Œæ ‡ç­¾æ˜¯å¦åŒ…å«ä»»åŠ¡å…³é”®è¯
        
        Args:
            url: ç”¨æˆ·æäº¤çš„è§†é¢‘é“¾æ¥
            task_title: ä»»åŠ¡æ ‡é¢˜ï¼ˆç”¨äºå…³é”®è¯åŒ¹é…ï¼‰
            task_description: ä»»åŠ¡æè¿°ï¼ˆç”¨äºå…³é”®è¯åŒ¹é…ï¼‰
            timeout: é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        
        Returns:
            dict: {
                'success': bool,  # éªŒè¯æ˜¯å¦æˆåŠŸ
                'matched': bool,  # æ˜¯å¦åŒ¹é…ä»»åŠ¡å…³é”®è¯
                'screenshot_path': str,  # æˆªå›¾è·¯å¾„
                'page_title': str,  # é¡µé¢æ ‡é¢˜
                'page_text': str,  # é¡µé¢æ–‡æœ¬å†…å®¹ï¼ˆæè¿°+æ ‡ç­¾ï¼‰
                'error': str  # é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            }
        """
        result = {
            'success': False,
            'matched': False,
            'screenshot_path': None,
            'page_title': '',
            'page_text': '',
            'error': None
        }
        
        try:
            logger.info(f"ğŸ” å¼€å§‹éªŒè¯é“¾æ¥: {url}")
            logger.info("ğŸ­ Step 1: å¯åŠ¨ Playwright...")
            
            async with async_playwright() as p:
                logger.info("âœ… Playwright å·²å¯åŠ¨")
                # å¯åŠ¨æµè§ˆå™¨ï¼ˆä½¿ç”¨ chromiumï¼‰
                logger.info("ğŸ­ Step 2: å¯åŠ¨ Chromium æµè§ˆå™¨...")
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage']
                )
                logger.info("âœ… Chromium æµè§ˆå™¨å·²å¯åŠ¨")
                
                logger.info("ğŸ­ Step 3: åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡...")
                context = await browser.new_context(
                    viewport={'width': 1280, 'height': 720},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                logger.info("âœ… ä¸Šä¸‹æ–‡å·²åˆ›å»º")
                
                logger.info("ğŸ­ Step 4: åˆ›å»ºæ–°é¡µé¢...")
                page = await context.new_page()
                logger.info("âœ… é¡µé¢å·²åˆ›å»º")
                
                # è®¿é—®é“¾æ¥
                logger.info(f"ğŸ­ Step 5: è®¿é—®é¡µé¢ {url}...")
                try:
                    await page.goto(url, timeout=15000, wait_until='networkidle')
                    logger.info("âœ… é¡µé¢åŠ è½½å®Œæˆ")
                except PlaywrightTimeout:
                    logger.warning("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå°è¯•ä½¿ç”¨ domcontentloaded...")
                    try:
                        await page.goto(url, timeout=10000, wait_until='domcontentloaded')
                    except Exception as e2:
                        logger.error(f"é¡µé¢åŠ è½½å¤±è´¥: {e2}")
                        result['error'] = f"æ— æ³•è®¿é—®é“¾æ¥: {str(e2)}"
                        return result
                except Exception as e:
                    logger.error(f"è®¿é—®é¡µé¢å¤±è´¥: {e}")
                    result['error'] = f"æ— æ³•è®¿é—®é“¾æ¥: {str(e)}"
                    return result
                
                # ç­‰å¾…é¡µé¢æ¸²æŸ“ï¼ˆTikTok éœ€è¦æ—¶é—´åŠ è½½åŠ¨æ€å†…å®¹ï¼‰
                logger.info("ğŸ­ Step 6: ç­‰å¾…é¡µé¢æ¸²æŸ“...")
                try:
                    await page.wait_for_timeout(3000)
                    logger.info("âœ… é¡µé¢æ¸²æŸ“å®Œæˆ")
                except Exception as e:
                    logger.warning(f"ç­‰å¾…è¶…æ—¶: {e}")
                
                # è·å–é¡µé¢æ ‡é¢˜
                logger.info("ğŸ­ Step 7: è·å–é¡µé¢æ ‡é¢˜...")
                result['page_title'] = await page.title()
                logger.info(f"âœ… é¡µé¢æ ‡é¢˜: {result['page_title']}")
                
                # æå–è§†é¢‘æè¿°å’Œæ ‡ç­¾
                logger.info("ğŸ­ Step 8: æå–æè¿°å’Œæ ‡ç­¾...")
                try:
                    result['page_text'] = await self._extract_description_and_tags(page, url)
                    logger.info(f"ğŸ“ æå–åˆ°çš„æè¿°å’Œæ ‡ç­¾: {result['page_text'][:300] if result['page_text'] else '(ç©º)'}...")
                except Exception as e:
                    logger.error(f"æå–å†…å®¹å¤±è´¥: {e}")
                    result['page_text'] = ''
                
                # æˆªå›¾ä¿å­˜
                logger.info("ğŸ­ Step 9: æˆªå›¾...")
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                screenshot_filename = f"verify_{timestamp}.png"
                screenshot_path = os.path.join(self.screenshots_dir, screenshot_filename)
                
                try:
                    await page.screenshot(path=screenshot_path, full_page=False, timeout=10000)
                    result['screenshot_path'] = screenshot_path
                    logger.info(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                except Exception as e:
                    logger.warning(f"æˆªå›¾å¤±è´¥: {e}")
                
                logger.info("ğŸ­ Step 10: å…³é—­æµè§ˆå™¨...")
                try:
                    await browser.close()
                    logger.info("âœ… æµè§ˆå™¨å·²å…³é—­")
                except Exception as e:
                    logger.warning(f"å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")
                
                # éªŒè¯å…³é”®è¯åŒ¹é…ï¼ˆåªæ£€æŸ¥æè¿°å’Œæ ‡ç­¾ï¼‰
                result['matched'] = self._check_keywords_match(
                    result['page_text'],
                    task_title,
                    task_description
                )
                
                result['success'] = True
                logger.info(f"âœ… éªŒè¯å®Œæˆï¼ŒåŒ¹é…ç»“æœ: {result['matched']}")
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}", exc_info=True)
            result['error'] = str(e)
        
        return result
    
    async def _extract_description_and_tags(self, page, url: str) -> str:
        """
        æå–è§†é¢‘æè¿°å’Œæ ‡ç­¾
        
        Args:
            page: Playwright page å¯¹è±¡
            url: è§†é¢‘é“¾æ¥
        
        Returns:
            str: æè¿°å’Œæ ‡ç­¾çš„åˆå¹¶æ–‡æœ¬
        """
        text_parts = []
        
        # åˆ¤æ–­å¹³å°
        if 'tiktok.com' in url.lower():
            # TikTok ç‰¹å®šé€‰æ‹©å™¨
            selectors = [
                # è§†é¢‘æè¿°
                '[data-e2e="browse-video-desc"]',
                '[data-e2e="video-desc"]',
                'h1[data-e2e="browse-video-title"]',
                # Meta æ ‡ç­¾
                'meta[property="og:description"]',
                'meta[name="description"]',
            ]
            
            for selector in selectors:
                try:
                    if selector.startswith('meta'):
                        content = await page.get_attribute(selector, 'content', timeout=2000)
                        if content:
                            text_parts.append(content)
                            logger.info(f"âœ“ æå–åˆ° meta å†…å®¹: {content[:100]}")
                    else:
                        element = page.locator(selector).first
                        if await element.is_visible(timeout=2000):
                            text = await element.inner_text()
                            if text:
                                text_parts.append(text)
                                logger.info(f"âœ“ æå–åˆ°æè¿°: {text[:100]}")
                except Exception as e:
                    logger.debug(f"é€‰æ‹©å™¨ {selector} æœªæ‰¾åˆ°: {e}")
                    continue
            
            # æå–æ ‡ç­¾ï¼ˆhashtagsï¼‰
            try:
                hashtag_elements = await page.locator('a[href*="/tag/"]').all()
                hashtags = []
                for elem in hashtag_elements[:20]:  # é™åˆ¶æœ€å¤š20ä¸ªæ ‡ç­¾
                    try:
                        tag_text = await elem.inner_text()
                        if tag_text.startswith('#'):
                            hashtags.append(tag_text)
                    except:
                        continue
                
                if hashtags:
                    hashtag_text = ' '.join(hashtags)
                    text_parts.append(hashtag_text)
                    logger.info(f"âœ“ æå–åˆ°æ ‡ç­¾: {hashtag_text}")
            except Exception as e:
                logger.debug(f"æå–æ ‡ç­¾å¤±è´¥: {e}")
        
        elif 'youtube.com' in url.lower() or 'youtu.be' in url.lower():
            # YouTube ç‰¹å®šé€‰æ‹©å™¨
            selectors = [
                '#title h1',
                'yt-formatted-string.ytd-video-primary-info-renderer',
                'meta[property="og:title"]',
                'meta[property="og:description"]',
                'meta[name="description"]',
            ]
            
            for selector in selectors:
                try:
                    if selector.startswith('meta'):
                        content = await page.get_attribute(selector, 'content', timeout=2000)
                        if content:
                            text_parts.append(content)
                    else:
                        element = page.locator(selector).first
                        text = await element.inner_text(timeout=2000)
                        if text:
                            text_parts.append(text)
                except:
                    continue
        
        elif 'instagram.com' in url.lower():
            # Instagram ç‰¹å®šé€‰æ‹©å™¨
            selectors = [
                'h1',
                'article h2',
                'meta[property="og:title"]',
                'meta[property="og:description"]',
            ]
            
            for selector in selectors:
                try:
                    if selector.startswith('meta'):
                        content = await page.get_attribute(selector, 'content', timeout=2000)
                        if content:
                            text_parts.append(content)
                    else:
                        element = page.locator(selector).first
                        text = await element.inner_text(timeout=2000)
                        if text:
                            text_parts.append(text)
                except:
                    continue
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹ï¼Œå°è¯•ä½¿ç”¨é¡µé¢æ ‡é¢˜
        if not text_parts:
            try:
                page_title = await page.title()
                if page_title:
                    logger.warning(f"âš ï¸ æœªæå–åˆ°æè¿°å’Œæ ‡ç­¾ï¼Œä½¿ç”¨é¡µé¢æ ‡é¢˜: {page_title}")
                    text_parts.append(page_title)
            except Exception as e:
                logger.error(f"è·å–é¡µé¢æ ‡é¢˜å¤±è´¥: {e}")
        
        return ' '.join(text_parts)
    
    def _check_keywords_match(self, page_text: str, task_title: str, task_description: str) -> bool:
        """
        æ£€æŸ¥æè¿°å’Œæ ‡ç­¾æ˜¯å¦åŒ…å«ä»»åŠ¡å…³é”®è¯
        
        Args:
            page_text: é¡µé¢æ–‡æœ¬ï¼ˆæè¿°+æ ‡ç­¾ï¼‰
            task_title: ä»»åŠ¡æ ‡é¢˜
            task_description: ä»»åŠ¡æè¿°
        
        Returns:
            bool: æ˜¯å¦åŒ¹é…
        """
        # è½¬æ¢ä¸ºå°å†™
        page_content = page_text.lower()
        
        # ä»ä»»åŠ¡æ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼ˆä¸ä½¿ç”¨æè¿°ï¼‰
        keywords = set()
        
        # æå–ä»»åŠ¡æ ‡é¢˜ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
        title_words = re.findall(r'[\w\u4e00-\u9fff]+', task_title)
        # è¿‡æ»¤æ‰å•å­—ã€çº¯æ•°å­—ã€åŒ…å«æ•°å­—çš„è¯å’Œå¸¸è§è¯
        for w in title_words:
            if len(w) <= 1:  # è¿‡æ»¤å•å­—
                continue
            if w.isdigit():  # è¿‡æ»¤çº¯æ•°å­—
                continue
            if re.search(r'\d', w):  # è¿‡æ»¤åŒ…å«æ•°å­—çš„è¯ï¼ˆå¦‚"ç¬¬5é›†"ã€"è§‰é†’2"ï¼‰
                continue
            keywords.add(w.lower())
        
        # ä¸å†ä»æè¿°ä¸­æå–å…³é”®è¯ï¼Œå› ä¸ºæè¿°åŒ…å«å¤§é‡è¥é”€æ€§è¯è¯­
        
        # ç§»é™¤å¸¸è§åœç”¨è¯å’Œè¥é”€è¯è¯­
        stopwords = {
            # åŸºç¡€åœç”¨è¯
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”',
            'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but', 'in', 'to', 'of', 'for',
            # å•ä¸ªè¥é”€è¯
            'æ¨è', 'è§‚çœ‹', 'è¿™éƒ¨', 'ç²¾å½©', 'çŸ­å‰§', 'ç‰‡æ®µ', 'å‰§æƒ…', 'è·Œå®•èµ·ä¼', 'ä¸å®¹é”™è¿‡',
            'ç²¾é€‰', 'çƒ­é—¨', 'å¥½çœ‹', 'å¿…çœ‹', 'å¼ºçƒˆæ¨è', 'çƒ­æ’­', 'çˆ†æ¬¾', 'çƒ­å‰§',
            'å¥³ä¸»', 'ç”·ä¸»', 'è§’è‰²', 'è§‰é†’', 'é€†è¢­', 'å¤ä»‡', 'é‡ç”Ÿ', 'ç©¿è¶Š',
            'ç¬¬', 'é›†', 'ep', 'episode',
            # ç»„åˆè¥é”€è¯
            'ç²¾é€‰çŸ­å‰§', 'çƒ­é—¨çŸ­å‰§', 'å¥³ä¸»è§‰é†’', 'ç”·ä¸»é€†è¢­', 'å¥³ä¸»é€†è¢­', 'å¥³ä¸»å¤ä»‡', 'ç”·ä¸»å¤ä»‡',
            'å¥³ä¸»é‡ç”Ÿ', 'ç”·ä¸»é‡ç”Ÿ', 'å¥³ä¸»ç©¿è¶Š', 'ç”·ä¸»ç©¿è¶Š', 'å¥³å¼ºç”·å¼º',
            # å…¶ä»–å¸¸è§è¯
            'ä¸­', 'ä¸Š', 'ä¸‹', 'å·¦', 'å³', 'å‰', 'å'
        }
        keywords = keywords - stopwords
        
        # å†æ¬¡è¿‡æ»¤ï¼šåˆ é™¤åŒ…å«åœç”¨è¯çš„ç»„åˆè¯
        keywords_to_remove = set()
        for keyword in keywords:
            # å¦‚æœå…³é”®è¯åŒ…å«ä»»ä½•åœç”¨è¯ï¼Œåˆ é™¤å®ƒ
            for stopword in ['çŸ­å‰§', 'ç²¾é€‰', 'çƒ­é—¨', 'å¥³ä¸»', 'ç”·ä¸»', 'è§‰é†’', 'é€†è¢­', 'å¤ä»‡', 'é‡ç”Ÿ', 'ç©¿è¶Š']:
                if stopword in keyword:
                    keywords_to_remove.add(keyword)
                    break
        keywords = keywords - keywords_to_remove
        
        logger.info(f"ğŸ”‘ å…³é”®è¯åˆ—è¡¨: {keywords}")
        
        # æ£€æŸ¥è‡³å°‘åŒ¹é… 30% çš„å…³é”®è¯
        if not keywords:
            logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œé»˜è®¤é€šè¿‡")
            return True
        
        matched_count = sum(1 for keyword in keywords if keyword in page_content)
        match_rate = matched_count / len(keywords)
        
        logger.info(f"ğŸ“Š åŒ¹é…ç‡: {match_rate:.2%} ({matched_count}/{len(keywords)})")
        logger.info(f"ğŸ“‹ åŒ¹é…çš„å…³é”®è¯: {[k for k in keywords if k in page_content]}")
        logger.info(f"âŒ æœªåŒ¹é…çš„å…³é”®è¯: {[k for k in keywords if k not in page_content]}")
        
        # è‡³å°‘åŒ¹é… 20% çš„å…³é”®è¯æ‰ç®—é€šè¿‡ï¼ˆé™ä½é˜ˆå€¼å› ä¸ºåªä½¿ç”¨æ ‡é¢˜å…³é”®è¯ï¼‰
        return match_rate >= 0.2


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    import asyncio
    
    logging.basicConfig(level=logging.INFO)
    
    async def test():
        verifier = LinkVerifier()
        
        # æµ‹è¯• TikTok é“¾æ¥
        result = await verifier.verify_link(
            url="https://www.tiktok.com/@wu.roger7/video/7577128093949725966",
            task_title="å…»æ¯èƒœè¿‡ç”Ÿæ¯",
            task_description="æ¨èè§‚çœ‹è¿™éƒ¨ç²¾å½©çŸ­å‰§ç‰‡æ®µã€Šå…»æ¯èƒœè¿‡ç”Ÿæ¯ã€‹ï¼Œå‰§æƒ…è·Œå®•èµ·ä¼ï¼Œä¸å®¹é”™è¿‡ï¼"
        )
        
        print("\néªŒè¯ç»“æœ:")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"åŒ¹é…: {result['matched']}")
        print(f"æ ‡é¢˜: {result['page_title']}")
        print(f"æè¿°å’Œæ ‡ç­¾: {result['page_text']}")
        print(f"æˆªå›¾: {result['screenshot_path']}")
        if result['error']:
            print(f"é”™è¯¯: {result['error']}")
    
    asyncio.run(test())
