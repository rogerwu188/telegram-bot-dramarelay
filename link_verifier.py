#!/usr/bin/env python3
"""
é“¾æ¥éªŒè¯æ¨¡å—ï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼‰
ä½¿ç”¨ requests + BeautifulSoup éªŒè¯è§†é¢‘é“¾æ¥å†…å®¹
"""
import logging
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class LinkVerifier:
    """è§†é¢‘é“¾æ¥éªŒè¯å™¨ï¼ˆè½»é‡çº§ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        })
    
    def verify_link(self, url: str, task_title: str, task_description: str, timeout: int = 15) -> dict:
        """
        éªŒè¯è§†é¢‘é“¾æ¥
        
        Args:
            url: ç”¨æˆ·æäº¤çš„è§†é¢‘é“¾æ¥
            task_title: ä»»åŠ¡æ ‡é¢˜ï¼ˆç”¨äºå…³é”®è¯åŒ¹é…ï¼‰
            task_description: ä»»åŠ¡æè¿°ï¼ˆç”¨äºå…³é”®è¯åŒ¹é…ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            dict: {
                'success': bool,  # éªŒè¯æ˜¯å¦æˆåŠŸ
                'matched': bool,  # æ˜¯å¦åŒ¹é…ä»»åŠ¡å…³é”®è¯
                'page_title': str,  # é¡µé¢æ ‡é¢˜
                'page_text': str,  # é¡µé¢æ–‡æœ¬å†…å®¹
                'error': str  # é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            }
        """
        result = {
            'success': False,
            'matched': False,
            'page_title': '',
            'page_text': '',
            'error': None
        }
        
        try:
            logger.info(f"ğŸ” å¼€å§‹éªŒè¯é“¾æ¥: {url}")
            
            # å‘é€ HTTP è¯·æ±‚
            response = self.session.get(url, timeout=timeout, allow_redirects=True)
            response.raise_for_status()
            
            logger.info(f"âœ… æˆåŠŸè·å–é¡µé¢ï¼ŒçŠ¶æ€ç : {response.status_code}")
            
            # è§£æ HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–é¡µé¢æ ‡é¢˜
            result['page_title'] = self._extract_title(soup, url)
            logger.info(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result['page_title']}")
            
            # æå–é¡µé¢æè¿°å’Œå†…å®¹
            result['page_text'] = self._extract_content(soup, url)
            logger.info(f"ğŸ“ æå–åˆ°çš„æ–‡æœ¬: {result['page_text'][:200]}...")
            
            # éªŒè¯å…³é”®è¯åŒ¹é…
            result['matched'] = self._check_keywords_match(
                result['page_title'],
                result['page_text'],
                task_title,
                task_description
            )
            
            result['success'] = True
            logger.info(f"âœ… éªŒè¯å®Œæˆï¼ŒåŒ¹é…ç»“æœ: {result['matched']}")
            
        except requests.exceptions.Timeout:
            logger.error(f"âŒ è¯·æ±‚è¶…æ—¶")
            result['error'] = "è¯·æ±‚è¶…æ—¶ï¼Œæ— æ³•è®¿é—®é“¾æ¥"
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            result['error'] = f"æ— æ³•è®¿é—®é“¾æ¥: {str(e)}"
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            result['error'] = str(e)
        
        return result
    
    def _extract_title(self, soup: BeautifulSoup, url: str) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        # ä¼˜å…ˆçº§é¡ºåºï¼šog:title > twitter:title > title æ ‡ç­¾
        
        # 1. Open Graph title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content'].strip()
        
        # 2. Twitter title
        twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
        if twitter_title and twitter_title.get('content'):
            return twitter_title['content'].strip()
        
        # 3. æ ‡å‡† title æ ‡ç­¾
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        # 4. å¹³å°ç‰¹å®šé€‰æ‹©å™¨
        domain = urlparse(url).netloc.lower()
        
        if 'tiktok.com' in domain:
            # TikTok ç‰¹å®šé€‰æ‹©å™¨
            desc = soup.find('meta', attrs={'name': 'description'})
            if desc and desc.get('content'):
                return desc['content'].strip()
        
        return ""
    
    def _extract_content(self, soup: BeautifulSoup, url: str) -> str:
        """æå–é¡µé¢å†…å®¹"""
        content_parts = []
        
        # 1. Meta æè¿°
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            content_parts.append(meta_desc['content'])
        
        # 2. Open Graph æè¿°
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            content_parts.append(og_desc['content'])
        
        # 3. Twitter æè¿°
        twitter_desc = soup.find('meta', attrs={'name': 'twitter:description'})
        if twitter_desc and twitter_desc.get('content'):
            content_parts.append(twitter_desc['content'])
        
        # 4. å¹³å°ç‰¹å®šå†…å®¹æå–
        domain = urlparse(url).netloc.lower()
        
        if 'tiktok.com' in domain:
            # TikTok: å°è¯•ä» JSON-LD æå–
            json_ld = soup.find('script', type='application/ld+json')
            if json_ld:
                content_parts.append(json_ld.string)
        
        elif 'youtube.com' in domain or 'youtu.be' in domain:
            # YouTube: æå–è§†é¢‘æè¿°
            yt_desc = soup.find('meta', attrs={'name': 'description'})
            if yt_desc and yt_desc.get('content'):
                content_parts.append(yt_desc['content'])
        
        elif 'instagram.com' in domain:
            # Instagram: æå– og:description
            pass  # å·²åœ¨ä¸Šé¢å¤„ç†
        
        # 5. æå–ä¸»è¦æ–‡æœ¬å†…å®¹ï¼ˆh1, h2, p æ ‡ç­¾ï¼‰
        for tag in soup.find_all(['h1', 'h2', 'p'], limit=10):
            text = tag.get_text().strip()
            if text and len(text) > 10:
                content_parts.append(text)
        
        return ' '.join(content_parts)
    
    def _check_keywords_match(self, page_title: str, page_text: str, task_title: str, task_description: str) -> bool:
        """
        æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦åŒ…å«ä»»åŠ¡å…³é”®è¯
        
        Args:
            page_title: é¡µé¢æ ‡é¢˜
            page_text: é¡µé¢æ–‡æœ¬
            task_title: ä»»åŠ¡æ ‡é¢˜
            task_description: ä»»åŠ¡æè¿°
        
        Returns:
            bool: æ˜¯å¦åŒ¹é…
        """
        # åˆå¹¶é¡µé¢å†…å®¹
        page_content = f"{page_title} {page_text}".lower()
        
        # ä»ä»»åŠ¡æ ‡é¢˜å’Œæè¿°ä¸­æå–å…³é”®è¯
        keywords = set()
        
        # æå–ä»»åŠ¡æ ‡é¢˜ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
        title_words = re.findall(r'[\w\u4e00-\u9fff]+', task_title)
        # è¿‡æ»¤æ‰å•å­—å’Œå¸¸è§è¯
        keywords.update([w.lower() for w in title_words if len(w) > 1])
        
        # æå–ä»»åŠ¡æè¿°ä¸­çš„å…³é”®è¯
        if task_description:
            desc_words = re.findall(r'[\w\u4e00-\u9fff]+', task_description)
            keywords.update([w.lower() for w in desc_words if len(w) > 1])
        
        # ç§»é™¤å¸¸è§åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 
                     'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but'}
        keywords = keywords - stopwords
        
        logger.info(f"ğŸ”‘ å…³é”®è¯åˆ—è¡¨: {keywords}")
        
        # æ£€æŸ¥è‡³å°‘åŒ¹é… 30% çš„å…³é”®è¯
        if not keywords:
            logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œé»˜è®¤é€šè¿‡")
            return True
        
        matched_count = sum(1 for keyword in keywords if keyword in page_content)
        match_rate = matched_count / len(keywords)
        
        logger.info(f"ğŸ“Š åŒ¹é…ç‡: {match_rate:.2%} ({matched_count}/{len(keywords)})")
        logger.info(f"ğŸ“‹ åŒ¹é…çš„å…³é”®è¯: {[k for k in keywords if k in page_content]}")
        
        # è‡³å°‘åŒ¹é… 30% çš„å…³é”®è¯æ‰ç®—é€šè¿‡
        return match_rate >= 0.3


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    
    verifier = LinkVerifier()
    
    # æµ‹è¯• TikTok é“¾æ¥
    result = verifier.verify_link(
        url="https://www.tiktok.com/@wu.roger7/video/7576774823712394551",
        task_title="å…»æ¯èƒœè¿‡ç”Ÿæ¯",
        task_description="åˆ†äº«çŸ­å‰§ã€Šå…»æ¯èƒœè¿‡ç”Ÿæ¯ã€‹çœŸæƒ…åè½¬ç‰‡æ®µ"
    )
    
    print("\néªŒè¯ç»“æœ:")
    print(f"æˆåŠŸ: {result['success']}")
    print(f"åŒ¹é…: {result['matched']}")
    print(f"æ ‡é¢˜: {result['page_title']}")
    if result['error']:
        print(f"é”™è¯¯: {result['error']}")
